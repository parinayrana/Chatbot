from fastapi import FastAPI, Depends, HTTPException, Header
import ollama
from dotenv import load_dotenv

app = FastAPI()
# initiating the api






@app.post("/generate")
def generate(prompt: str):
    reponse = ollama.chat(model='mistral', messages = [{'role':"user", "content": prompt}])   #the messages dict is the input to the model giving it instructions on what and how to do 
    #  for example messages = [{'role':"system", 'content':"You are a poetic assistant who answers in rhymes."},{'role':"user", "content": prompt}])
    return {"response": reponse['message']['content']}
    # the response generated by model generally follow the same format where in a dictiory various things are given in which the message dict is there containing the main output 



